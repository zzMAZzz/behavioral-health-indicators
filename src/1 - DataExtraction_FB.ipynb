{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1579cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURACI√ìN E IMPORTS\n",
    "# ============================================================\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar la ra√≠z del proyecto al path\n",
    "ROOT = Path().resolve().parent\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Importar configuraci√≥n centralizada\n",
    "from config import PATHS, CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec745b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= CONFIGURACI√ìN USANDO CONFIG.PY =============\n",
    "\n",
    "# Usar rutas centralizadas\n",
    "CARPETA_DATOS_CRUDOS = PATHS.RAW_FB\n",
    "CARPETA_PROCESADOS = PATHS.UNCLEANED\n",
    "\n",
    "# Usar configuraci√≥n centralizada\n",
    "PREFIJO_ID = CONFIG.PREFIJO_ID\n",
    "NUMERO_INICIAL = CONFIG.NUMERO_INICIAL\n",
    "USAR_FILTRO_FECHA = CONFIG.USAR_FILTRO_FECHA\n",
    "MESES_ATRAS = CONFIG.MESES_ATRAS\n",
    "FECHA_DESDE = CONFIG.FECHA_DESDE\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CONFIGURACI√ìN ACTIVA:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Datos crudos: {CARPETA_DATOS_CRUDOS}\")\n",
    "print(f\"Procesados: {CARPETA_PROCESADOS}\")\n",
    "print(f\"Prefijo ID: {PREFIJO_ID}\")\n",
    "print(f\"Filtro de fecha: {'Activo' if USAR_FILTRO_FECHA else 'Inactivo'}\")\n",
    "if USAR_FILTRO_FECHA:\n",
    "    if FECHA_DESDE:\n",
    "        print(f\"  Desde: {FECHA_DESDE}\")\n",
    "    else:\n",
    "        print(f\"  √öltimos {MESES_ATRAS} meses\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "097004e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_fecha(fecha_str):\n",
    "    \"\"\"Normaliza diferentes formatos de fecha a YYYY-MM-DD HH:MM:SS\"\"\"\n",
    "    if pd.isna(fecha_str) or fecha_str == \"\":\n",
    "        return \"\"\n",
    "    \n",
    "    if isinstance(fecha_str, datetime):\n",
    "        return fecha_str.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    fecha_str = str(fecha_str).strip()\n",
    "    \n",
    "    try:\n",
    "        # Formato ISO con timezone (2023-01-16T11:34:28Z)\n",
    "        if 'T' in fecha_str:\n",
    "            fecha_str = fecha_str.replace('Z', '')\n",
    "            dt = datetime.fromisoformat(fecha_str.split('+')[0].split('-0')[0])\n",
    "            return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Formato WhatsApp espa√±ol: \"27/2/25 1:42 p. m.\"\n",
    "        if 'p. m.' in fecha_str or 'a. m.' in fecha_str:\n",
    "            fecha_str_limpia = fecha_str.replace('p. m.', 'PM').replace('a. m.', 'AM')\n",
    "            partes = fecha_str_limpia.split()\n",
    "            \n",
    "            if len(partes) >= 2:\n",
    "                fecha_nums = partes[0].split('/')\n",
    "                dia, mes, anio = int(fecha_nums[0]), int(fecha_nums[1]), int(fecha_nums[2])\n",
    "                \n",
    "                if anio < 100:\n",
    "                    anio = 2000 + anio if anio < 50 else 1900 + anio\n",
    "                \n",
    "                hora_nums = partes[1].split(':')\n",
    "                hora = int(hora_nums[0])\n",
    "                minuto = int(hora_nums[1]) if len(hora_nums) > 1 else 0\n",
    "                am_pm = partes[2] if len(partes) > 2 else \"AM\"\n",
    "                \n",
    "                if am_pm == 'PM' and hora != 12:\n",
    "                    hora += 12\n",
    "                elif am_pm == 'AM' and hora == 12:\n",
    "                    hora = 0\n",
    "                \n",
    "                dt = datetime(anio, mes, dia, hora, minuto, 0)\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Formato M/D/YYYY (7/17/2025)\n",
    "        if '/' in fecha_str and len(fecha_str.split('/')) == 3:\n",
    "            parts = fecha_str.split('/')\n",
    "            if len(parts[2]) == 4:\n",
    "                dt = datetime.strptime(fecha_str, '%m/%d/%Y')\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Meses en espa√±ol\n",
    "        meses_es = {\n",
    "            'ene': 'Jan', 'feb': 'Feb', 'mar': 'Mar', 'abr': 'Apr',\n",
    "            'may': 'May', 'jun': 'Jun', 'jul': 'Jul', 'ago': 'Aug',\n",
    "            'sep': 'Sep', 'oct': 'Oct', 'nov': 'Nov', 'dic': 'Dec'\n",
    "        }\n",
    "        \n",
    "        fecha_lower = fecha_str.lower()\n",
    "        for mes_es, mes_en in meses_es.items():\n",
    "            if mes_es in fecha_lower:\n",
    "                fecha_str = fecha_str.lower().replace(mes_es, mes_en)\n",
    "                break\n",
    "        \n",
    "        # Formatos comunes\n",
    "        formatos = [\n",
    "            '%b %d, %Y %I:%M:%S %p',\n",
    "            '%B %d, %Y %I:%M:%S %p',\n",
    "            '%Y-%m-%d %H:%M:%S',\n",
    "            '%d/%m/%Y %H:%M:%S',\n",
    "            '%m/%d/%Y %H:%M:%S',\n",
    "            '%Y-%m-%d',\n",
    "            '%d/%m/%Y',\n",
    "            '%m/%d/%Y',\n",
    "        ]\n",
    "        \n",
    "        for formato in formatos:\n",
    "            try:\n",
    "                dt = datetime.strptime(fecha_str, formato)\n",
    "                return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return fecha_str\n",
    "        \n",
    "    except Exception as e:\n",
    "        return fecha_str\n",
    "    \n",
    "def aplicar_filtro_fecha(fecha_str, fecha_limite):\n",
    "    \"\"\"Verifica si una fecha cumple con el filtro establecido\"\"\"\n",
    "    if not fecha_limite or not fecha_str:\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        fecha_dt = datetime.strptime(fecha_str, '%Y-%m-%d %H:%M:%S')\n",
    "        return fecha_dt >= fecha_limite\n",
    "    except:\n",
    "        return True  # Si hay error, incluir el registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7628a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_html(archivo_html, id_participante, fecha_limite=None):\n",
    "    \"\"\"Extrae publicaciones de archivos HTML (Facebook, Instagram, etc.)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(archivo_html, 'r', encoding='utf-8') as file:\n",
    "            html_content = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo archivo: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    sections = soup.find_all('section', class_='_a6-g')\n",
    "    posts_data = []\n",
    "    \n",
    "    for idx, section in enumerate(sections, 1):\n",
    "        # Extraer fecha\n",
    "        fecha_publicacion = None\n",
    "        footer = section.find('footer')\n",
    "        time_elem = footer.find('time') if footer else None\n",
    "        \n",
    "        if time_elem:\n",
    "            fecha_publicacion = time_elem.get('datetime')\n",
    "        else:\n",
    "            date_div = section.find('div', string=re.compile(r'Actualizado'))\n",
    "            if date_div:\n",
    "                fecha_publicacion = date_div.text.replace('Actualizado ', '')\n",
    "            elif footer:\n",
    "                date_div = footer.find('div', class_='_a72d')\n",
    "                if date_div:\n",
    "                    fecha_publicacion = date_div.text.strip()\n",
    "        \n",
    "        # Normalizar fecha\n",
    "        fecha_normalizada = normalizar_fecha(fecha_publicacion)\n",
    "        \n",
    "        # Aplicar filtro de fecha\n",
    "        if not aplicar_filtro_fecha(fecha_normalizada, fecha_limite):\n",
    "            continue\n",
    "        \n",
    "        # Extraer texto\n",
    "        texto_publicacion = \"\"\n",
    "        content_div = section.find('div', class_='_2ph_')\n",
    "        \n",
    "        if content_div:\n",
    "            text_divs = content_div.find_all('div', recursive=True)\n",
    "            for div in text_divs:\n",
    "                text = div.string\n",
    "                if text and text.strip() and not text.startswith('Actualizado'):\n",
    "                    if len(text.strip()) > 10:\n",
    "                        texto_publicacion = text.strip()\n",
    "                        break\n",
    "            \n",
    "            if not texto_publicacion:\n",
    "                link_elem = content_div.find('a')\n",
    "                if link_elem and link_elem.get('href'):\n",
    "                    texto_publicacion = link_elem.get('href')\n",
    "        \n",
    "        # Extraer tipo de publicaci√≥n\n",
    "        h2 = section.find('h2')\n",
    "        tipo_publicacion = h2.text.strip() if h2 else \"\"\n",
    "        \n",
    "        posts_data.append({\n",
    "            'id_participante': id_participante,\n",
    "            'id_publicacion': f\"{id_participante}_POST_{idx:03d}\",\n",
    "            'fuente': 'Facebook',\n",
    "            'fecha_publicacion': fecha_normalizada,\n",
    "            'texto_publicacion': texto_publicacion if texto_publicacion else tipo_publicacion\n",
    "        })\n",
    "    \n",
    "    return posts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_csv_excel(archivo, id_participante, fecha_limite=None):\n",
    "    \"\"\"Extrae publicaciones de archivos CSV o Excel\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Detectar tipo de archivo\n",
    "        if archivo.endswith(('.xlsx', '.xls')):\n",
    "            df = pd.read_excel(archivo, engine='openpyxl')\n",
    "        else:\n",
    "            # Intentar diferentes encodings\n",
    "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                try:\n",
    "                    df = pd.read_csv(archivo, encoding=encoding)\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error leyendo archivo: {e}\")\n",
    "        return []\n",
    "    \n",
    "    # Limpiar DataFrame\n",
    "    df = df.dropna(how='all')\n",
    "    \n",
    "    # Detectar columnas\n",
    "    col_fecha = None\n",
    "    col_texto = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower().strip()\n",
    "        if 'fecha' in col_lower or 'date' in col_lower:\n",
    "            col_fecha = col\n",
    "        if any(k in col_lower for k in ['publicacion', 'post', 'texto', 'text', 'content']):\n",
    "            col_texto = col\n",
    "    \n",
    "    if not col_fecha or not col_texto:\n",
    "        print(f\"  ‚ö†Ô∏è  Columnas no identificadas. Disponibles: {list(df.columns)}\")\n",
    "        return []\n",
    "    \n",
    "    posts_data = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        fecha_publicacion = row[col_fecha] if pd.notna(row[col_fecha]) else \"\"\n",
    "        texto_publicacion = row[col_texto] if pd.notna(row[col_texto]) else \"\"\n",
    "        \n",
    "        # Normalizar fecha\n",
    "        fecha_normalizada = normalizar_fecha(fecha_publicacion)\n",
    "        \n",
    "        # Aplicar filtro de fecha\n",
    "        if not aplicar_filtro_fecha(fecha_normalizada, fecha_limite):\n",
    "            continue\n",
    "        \n",
    "        if fecha_publicacion or texto_publicacion:\n",
    "            posts_data.append({\n",
    "                'id_participante': id_participante,\n",
    "                'id_publicacion': f\"{id_participante}_{idx+1:03d}\",\n",
    "                'fuente': 'Facebook',\n",
    "                'fecha_publicacion': fecha_normalizada,\n",
    "                'texto_publicacion': str(texto_publicacion)\n",
    "            })\n",
    "    \n",
    "    return posts_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f230a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_archivo(archivo, id_participante, fecha_limite=None):\n",
    "    \"\"\"Procesa un archivo y retorna las publicaciones extra√≠das\"\"\"\n",
    "    \n",
    "    archivo_lower = archivo.lower()\n",
    "    \n",
    "    # Detectar tipo de archivo\n",
    "    if archivo_lower.endswith(('.html', '.htm')):\n",
    "        return extraer_html(archivo, id_participante, fecha_limite)\n",
    "    elif archivo_lower.endswith(('.xlsx', '.xls', '.csv')):\n",
    "        return extraer_csv_excel(archivo, id_participante, fecha_limite)\n",
    "    else:\n",
    "        # Intentar detectar por contenido\n",
    "        try:\n",
    "            with open(archivo, 'r', encoding='utf-8') as f:\n",
    "                contenido = f.read(1000)\n",
    "                if '<html' in contenido.lower() or '<!doctype' in contenido.lower():\n",
    "                    return extraer_html(archivo, id_participante, fecha_limite)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Intentar como CSV\n",
    "        return extraer_csv_excel(archivo, id_participante, fecha_limite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_carpeta_completa():\n",
    "    \"\"\"Procesa todos los archivos en la carpeta de datos crudos\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXTRACTOR UNIVERSAL DE PUBLICACIONES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Crear carpeta de procesados si no existe\n",
    "    if not os.path.exists(CARPETA_PROCESADOS):\n",
    "        os.makedirs(CARPETA_PROCESADOS)\n",
    "        print(f\"‚úì Carpeta '{CARPETA_PROCESADOS}/' creada\")\n",
    "    \n",
    "    # Calcular fecha l√≠mite\n",
    "    fecha_limite = None\n",
    "    if USAR_FILTRO_FECHA:\n",
    "        if FECHA_DESDE:\n",
    "            fecha_limite = datetime.strptime(FECHA_DESDE, '%Y-%m-%d')\n",
    "            print(f\"\\nFiltro: Mensajes desde {FECHA_DESDE}\")\n",
    "        else:\n",
    "            fecha_limite = datetime.now() - timedelta(days=MESES_ATRAS * 30)\n",
    "            print(f\"\\nFiltro: √öltimos {MESES_ATRAS} meses (desde {fecha_limite.strftime('%Y-%m-%d')})\")\n",
    "    else:\n",
    "        print(\"\\nSin filtro de fecha\")\n",
    "    \n",
    "    # Verificar carpeta\n",
    "    if not os.path.exists(CARPETA_DATOS_CRUDOS):\n",
    "        print(f\"\\nError: La carpeta '{CARPETA_DATOS_CRUDOS}/' no existe\")\n",
    "        return None\n",
    "    \n",
    "    # Filtrar archivos: excluir archivos del sistema y ocultos\n",
    "    archivos_ignorar = {'.ds_store', 'thumbs.db', 'desktop.ini', '.gitkeep', '.gitignore'}\n",
    "    extensiones_validas = {'.html', '.htm', '.csv', '.xlsx', '.xls'}\n",
    "    \n",
    "    archivos = sorted([\n",
    "        f for f in os.listdir(CARPETA_DATOS_CRUDOS) \n",
    "        if os.path.isfile(os.path.join(CARPETA_DATOS_CRUDOS, f))\n",
    "        and not f.startswith('.')  # Ignorar archivos ocultos\n",
    "        and f.lower() not in archivos_ignorar  # Ignorar archivos del sistema\n",
    "        and any(f.lower().endswith(ext) for ext in extensiones_validas)  # Solo extensiones v√°lidas\n",
    "    ])\n",
    "    \n",
    "    if not archivos:\n",
    "        print(f\"\\n‚ö†Ô∏è  No se encontraron archivos v√°lidos en '{CARPETA_DATOS_CRUDOS}/'\")\n",
    "        print(f\"    Extensiones soportadas: {', '.join(extensiones_validas)}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüìÅ Archivos v√°lidos encontrados: {len(archivos)}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    archivos_guardados = []\n",
    "    contador = 0\n",
    "    estadisticas = []\n",
    "    \n",
    "    for nombre in archivos:\n",
    "        ruta = os.path.join(CARPETA_DATOS_CRUDOS, nombre)\n",
    "        contador += 1\n",
    "        \n",
    "        # Extraer n√∫mero del nombre del archivo\n",
    "        nombre_sin_ext = os.path.splitext(nombre)[0]\n",
    "        # Buscar n√∫meros en el nombre del archivo\n",
    "        numeros = re.findall(r'\\d+', nombre_sin_ext)\n",
    "        \n",
    "        if numeros:\n",
    "            # Usar el primer n√∫mero encontrado\n",
    "            numero = int(numeros[0])\n",
    "            id_participante = f\"{PREFIJO_ID}{numero:03d}\"\n",
    "        else:\n",
    "            # Si no hay n√∫mero, usar contador como fallback\n",
    "            id_participante = f\"{PREFIJO_ID}{NUMERO_INICIAL + contador - 1:03d}\"\n",
    "            print(f\"    ‚ö†Ô∏è  No se encontr√≥ n√∫mero en '{nombre}', usando contador\")\n",
    "        \n",
    "        print(f\"\\n[{contador}/{len(archivos)}] Procesando: {nombre}\")\n",
    "        print(f\"    ID Participante: {id_participante}\")\n",
    "        \n",
    "        try:\n",
    "            posts = procesar_archivo(ruta, id_participante, fecha_limite)\n",
    "            \n",
    "            if posts:\n",
    "                # Crear DataFrame para este participante\n",
    "                df_participante = pd.DataFrame(posts)\n",
    "                \n",
    "                # Generar nombre de archivo individual\n",
    "                nombre_archivo = f\"{id_participante}.csv\"\n",
    "                ruta_salida = os.path.join(CARPETA_PROCESADOS, nombre_archivo)\n",
    "\n",
    "                # Si existe un archivo previo, cargarlo y hacer merge seguro\n",
    "                if os.path.exists(ruta_salida):\n",
    "                    try:\n",
    "                        df_existente = pd.read_csv(ruta_salida, encoding='utf-8-sig')\n",
    "                        # Asegurar columna 'fuente' en existentes\n",
    "                        if 'fuente' not in df_existente.columns:\n",
    "                            df_existente['fuente'] = 'Unknown'\n",
    "                        inicio_idx = len(df_existente)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Error leyendo archivo existente: {e}. Se sobrescribir√° si es necesario.\")\n",
    "                        df_existente = pd.DataFrame()\n",
    "                        inicio_idx = 0\n",
    "                else:\n",
    "                    df_existente = pd.DataFrame()\n",
    "                    inicio_idx = 0\n",
    "\n",
    "                # Preparar claves para evitar duplicados (fecha + primeros 150 chars del texto)\n",
    "                if not df_existente.empty:\n",
    "                    df_existente['clave'] = df_existente['fecha_publicacion'].astype(str) + '_' + df_existente['texto_publicacion'].astype(str).str[:150]\n",
    "                \n",
    "                df_participante['fuente'] = 'Facebook'\n",
    "                df_participante['clave'] = df_participante['fecha_publicacion'].astype(str) + '_' + df_participante['texto_publicacion'].astype(str).str[:150]\n",
    "\n",
    "                # Filtrar publicaciones que ya existan en el archivo previo\n",
    "                if not df_existente.empty:\n",
    "                    df_participante = df_participante[~df_participante['clave'].isin(df_existente['clave'])]\n",
    "\n",
    "                # Si no hay nuevas publicaciones despu√©s del filtrado\n",
    "                if df_participante.empty:\n",
    "                    print(f\"    ‚ÑπÔ∏è  No hay publicaciones nuevas para {id_participante}\")\n",
    "                else:\n",
    "                    # Renumerar id_publicacion en los nuevos a partir de inicio_idx\n",
    "                    for i, row_idx in enumerate(df_participante.index):\n",
    "                        df_participante.at[row_idx, 'id_publicacion'] = f\"{id_participante}_{inicio_idx + i + 1:03d}\"\n",
    "\n",
    "                    # Concatenar y guardar\n",
    "                    if not df_existente.empty:\n",
    "                        df_final = pd.concat([df_existente.drop(columns=['clave'], errors='ignore'), df_participante.drop(columns=['clave'], errors='ignore')], ignore_index=True)\n",
    "                    else:\n",
    "                        df_final = df_participante.drop(columns=['clave'], errors='ignore')\n",
    "\n",
    "                    # Ordenar por id_participante (num√©rico si es posible) antes de guardar\n",
    "                    if 'id_participante' in df_final.columns:\n",
    "                        try:\n",
    "                            nums = df_final['id_participante'].astype(str).str.extract(r'(\\d+)$')[0]\n",
    "                            if nums.notna().any():\n",
    "                                df_final['_orden'] = nums.astype(float).astype('Int64')\n",
    "                                df_final = df_final.sort_values(['_orden', 'id_participante']).drop(columns=['_orden'])\n",
    "                            else:\n",
    "                                df_final = df_final.sort_values('id_participante')\n",
    "                        except Exception:\n",
    "                            df_final = df_final.sort_values('id_participante')\n",
    "\n",
    "                    df_final.to_csv(ruta_salida, index=False, encoding='utf-8-sig')\n",
    "\n",
    "                    print(f\"    ‚úì {len(df_participante)} publicaciones nuevas extra√≠das\")\n",
    "                    print(f\"    ‚úì Guardado como: {nombre_archivo} ({len(df_final)} total)\")\n",
    "\n",
    "                    archivos_guardados.append(nombre_archivo)\n",
    "                    estadisticas.append({\n",
    "                        'id_participante': id_participante,\n",
    "                        'archivo_original': nombre,\n",
    "                        'total_publicaciones': len(df_participante),\n",
    "                        'archivo_procesado': nombre_archivo\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"    Sin publicaciones\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    Error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not archivos_guardados:\n",
    "        print(\"\\nNo se extrajeron publicaciones de ning√∫n archivo\")\n",
    "        return None\n",
    "    \n",
    "    # Crear DataFrame con estad√≠sticas\n",
    "    df_estadisticas = pd.DataFrame(estadisticas)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úì PROCESO COMPLETADO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total de archivos procesados: {len(archivos_guardados)}\")\n",
    "    print(f\"Total de publicaciones extra√≠das: {df_estadisticas['total_publicaciones'].sum()}\")\n",
    "    print(f\"Ubicaci√≥n: {CARPETA_PROCESADOS}/\")\n",
    "    \n",
    "    # Resumen por participante\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"RESUMEN POR PARTICIPANTE:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(df_estadisticas[['id_participante', 'total_publicaciones', 'archivo_procesado']].to_string(index=False))\n",
    "    \n",
    "    # Lista de archivos generados\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ARCHIVOS GENERADOS:\")\n",
    "    print(\"-\" * 80)\n",
    "    for archivo in archivos_guardados:\n",
    "        print(f\"  üìÑ {archivo}\")\n",
    "    \n",
    "    return df_estadisticas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c21fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df_resultado = procesar_carpeta_completa()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (venv-ipykernel)",
   "language": "python",
   "name": "venv-ipykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
